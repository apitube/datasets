<?xml version="1.0" encoding="utf-8"?>
<response><creator>APITUBE (https://apitube.pub)</creator><total_count>10</total_count><updated_at>2022-11-03T08:40:01+00:00</updated_at><items><item><title>This Tech Creates High-End Stylized Digital Avatars</title><link>https://80.lv/articles/this-tech-creates-high-end-stylized-digital-avatars</link><created_at>2022-11-03T08:40:01+00:00</created_at><description>Pinscreen CEO Hao Li told about the company's AI-powered technology that creates high-quality stylized digital avatars doing a volumetric capture without the need of having multiple cameras.</description><content>&lt;div&gt;&lt;p&gt;In a video published by Amazon Web Services, Pinscreen CEO and founder Hao Li has spoken about the challenges involved in creating AI-driven, virtual human avatars and told about the company's technology that creates high-end stylized digital avatars.&lt;/p&gt;&lt;p&gt;Usually, the process of creating a high-quality digital human takes a long time, is pretty costly, and requires special facilities like capture settings. According to Li, Pinscreen created a technology that does a volumetric capture without the need of having multiple cameras. Instead, it can generate an avatar from a single photo.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;While a single camera that you have on your phone or a tablet captures the front side of a person, the backside is generated by the neural network which processes 3D information from a single viewpoint. Hao claims that this tech will allow users to stream a digital avatar from one location to another in real-time. &lt;/p&gt;&lt;p&gt;He also shared that the company is currently working on creating more realistic avatars. "We're trying to go away from, you know, emojis. We're trying to create something that is more natural to us," Hao said. "But overall, I think, we have a first result. I think it's still far from anything that's perfect but that's where we're at."&lt;/p&gt;&lt;p&gt;You can learn more about the technology by watching the video &lt;a href="https://www.youtube.com/watch?v=DoZFfUob82U" target="_blank"&gt;here&lt;/a&gt; or visiting Pinscreen's &lt;a href="https://www.pinscreen.com/" target="_blank"&gt;official website&lt;/a&gt;. Also, don't forget to join &lt;a href="https://www.reddit.com/r/80lv/" target="_blank"&gt;our new Reddit page&lt;/a&gt;, &lt;a href="https://t.me/LevelEightyNews" target="_blank"&gt;our new Telegram channel&lt;/a&gt;, follow us on &lt;a href="https://www.instagram.com/eighty_level/" target="_blank"&gt;Instagram&lt;/a&gt; and &lt;a href="https://twitter.com/80Level" target="_blank"&gt;Twitter&lt;/a&gt;, where we are sharing breakdowns, the latest news, awesome artworks, and more.&lt;/p&gt;&lt;/div&gt;</content><textplain>In a video published by Amazon Web Services, Pinscreen CEO and founder Hao Li has spoken about the challenges involved in creating AI-driven, virtual human avatars and told about the company's technology that creates high-end stylized digital avatars.Usually, the process of creating a high-quality digital human takes a long time, is pretty costly, and requires special facilities like capture settings. According to Li, Pinscreen created a technology that does a volumetric capture without the need of having multiple cameras. Instead, it can generate an avatar from a single photo.While a single camera that you have on your phone or a tablet captures the front side of a person, the backside is generated by the neural network which processes 3D information from a single viewpoint. Hao claims that this tech will allow users to stream a digital avatar from one location to another in real-time. He also shared that the company is currently working on creating more realistic avatars. "We're trying to go away from, you know, emojis. We're trying to create something that is more natural to us," Hao said. "But overall, I think, we have a first result. I think it's still far from anything that's perfect but that's where we're at."You can learn more about the technology by watching the video here or visiting Pinscreen's official website. Also, don't forget to join our new Reddit page, our new Telegram channel, follow us on Instagram and Twitter, where we are sharing breakdowns, the latest news, awesome artworks, and more.</textplain><image>https://cdn.80.lv/api/upload/meta/20079/images/627d0d582f0ee/contain_440x220.jpg</image></item><item><title>This Tech Creates High-End Stylized Digital Avatars</title><link>https://80.lv/articles/this-tech-creates-high-end-stylized-digital-avatars</link><created_at>2022-11-03T08:40:01+00:00</created_at><description>Pinscreen CEO Hao Li told about the company's AI-powered technology that creates high-quality stylized digital avatars doing a volumetric capture without the need of having multiple cameras.</description><content>&lt;div&gt;&lt;p&gt;In a video published by Amazon Web Services, Pinscreen CEO and founder Hao Li has spoken about the challenges involved in creating AI-driven, virtual human avatars and told about the company's technology that creates high-end stylized digital avatars.&lt;/p&gt;&lt;p&gt;Usually, the process of creating a high-quality digital human takes a long time, is pretty costly, and requires special facilities like capture settings. According to Li, Pinscreen created a technology that does a volumetric capture without the need of having multiple cameras. Instead, it can generate an avatar from a single photo.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;While a single camera that you have on your phone or a tablet captures the front side of a person, the backside is generated by the neural network which processes 3D information from a single viewpoint. Hao claims that this tech will allow users to stream a digital avatar from one location to another in real-time. &lt;/p&gt;&lt;p&gt;He also shared that the company is currently working on creating more realistic avatars. "We're trying to go away from, you know, emojis. We're trying to create something that is more natural to us," Hao said. "But overall, I think, we have a first result. I think it's still far from anything that's perfect but that's where we're at."&lt;/p&gt;&lt;p&gt;You can learn more about the technology by watching the video &lt;a href="https://www.youtube.com/watch?v=DoZFfUob82U" target="_blank"&gt;here&lt;/a&gt; or visiting Pinscreen's &lt;a href="https://www.pinscreen.com/" target="_blank"&gt;official website&lt;/a&gt;. Also, don't forget to join &lt;a href="https://www.reddit.com/r/80lv/" target="_blank"&gt;our new Reddit page&lt;/a&gt;, &lt;a href="https://t.me/LevelEightyNews" target="_blank"&gt;our new Telegram channel&lt;/a&gt;, follow us on &lt;a href="https://www.instagram.com/eighty_level/" target="_blank"&gt;Instagram&lt;/a&gt; and &lt;a href="https://twitter.com/80Level" target="_blank"&gt;Twitter&lt;/a&gt;, where we are sharing breakdowns, the latest news, awesome artworks, and more.&lt;/p&gt;&lt;/div&gt;</content><textplain>In a video published by Amazon Web Services, Pinscreen CEO and founder Hao Li has spoken about the challenges involved in creating AI-driven, virtual human avatars and told about the company's technology that creates high-end stylized digital avatars.Usually, the process of creating a high-quality digital human takes a long time, is pretty costly, and requires special facilities like capture settings. According to Li, Pinscreen created a technology that does a volumetric capture without the need of having multiple cameras. Instead, it can generate an avatar from a single photo.While a single camera that you have on your phone or a tablet captures the front side of a person, the backside is generated by the neural network which processes 3D information from a single viewpoint. Hao claims that this tech will allow users to stream a digital avatar from one location to another in real-time. He also shared that the company is currently working on creating more realistic avatars. "We're trying to go away from, you know, emojis. We're trying to create something that is more natural to us," Hao said. "But overall, I think, we have a first result. I think it's still far from anything that's perfect but that's where we're at."You can learn more about the technology by watching the video here or visiting Pinscreen's official website. Also, don't forget to join our new Reddit page, our new Telegram channel, follow us on Instagram and Twitter, where we are sharing breakdowns, the latest news, awesome artworks, and more.</textplain><image>https://cdn.80.lv/api/upload/meta/20079/images/627d0d582f0ee/contain_440x220.jpg</image></item><item><title>Tutorial: Generating Ocean Waves in Niagara &amp; UE5</title><link>https://80.lv/articles/tutorial-generating-ocean-waves-in-niagara-ue5</link><created_at>2022-11-03T08:39:04+00:00</created_at><description>If you were looking for a way to make your UE5-powered oceans and seas even more realistic, here's a new tutorial that will most certainly help you. Tech Art Director of Research and Development at Gearbox...</description><content>&lt;div&gt;&lt;p&gt;If you were looking for a way to make your UE5-powered oceans and seas even more realistic, here's a new tutorial that will most certainly help you. Tech Art Director of Research and Development at Gearbox and Founder of Overdraw.XYZ Ryan James Smith has released an in-depth breakdown that explains how to generate tiling ocean waves using Fast Fourier Transforms in Niagara and Unreal Engine 5. In this tutorial, the artist showed how to work with different maps, how to set up Fourier components, how to add foam and splashes, and more. You can check out the breakdown down below or by visiting&lt;a href="https://twitter.com/OverdrawXYZ" target="_blank"&gt;Ryan's Twitter page&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;</content><textplain>If you were looking for a way to make your UE5-powered oceans and seas even more realistic, here's a new tutorial that will most certainly help you. Tech Art Director of Research and Development at Gearbox and Founder of Overdraw.XYZ Ryan James Smith has released an in-depth breakdown that explains how to generate tiling ocean waves using Fast Fourier Transforms in Niagara and Unreal Engine 5. In this tutorial, the artist showed how to work with different maps, how to set up Fourier components, how to add foam and splashes, and more. You can check out the breakdown down below or by visitingRyan's Twitter page.</textplain><image>https://cdn.80.lv/api/upload/meta/20110/images/6281d01d1d9c3/contain_440x220.jpg</image></item><item><title>Tips on Using Blender for Concept Art</title><link>https://80.lv/articles/tips-on-using-blender-for-concept-art</link><created_at>2022-11-03T07:01:22+00:00</created_at><description>Nina Leinwatter has shared her experience and thoughts on the Environment Design Mentorship course, talked about the story behind the Gates of the Underworld project, and gave some useful tips on asset creation, rendering, and lighting in Blender.</description><content>&lt;div&gt;&lt;p&gt;Nina Leinwatter has shared her experience and thoughts on the Environment Design Mentorship course, talked about the story behind the Gates of the Underworld project, and gave some useful tips on asset creation, rendering, and lighting in Blender.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Introduction&lt;/h3&gt;&lt;p&gt;My Name is Nina Leinwatter, I am from Vienna, Austria. After I got my high school diploma in graphic design and fashion, I was studying Game Art and 3D Animation at the SAE Institute in Vienna. Then I started to focus on my portfolio and figured out what field of digital art I want to work in later on.&lt;/p&gt;&lt;p&gt;I was taking courses such as Keyframe Illustration for Production by Ricardo Lima, Concept Art for Film by Jama Jurabaev, and Environment Design Mentorship by Sathish Kumar. They all helped me a lot to choose my path and connect with people from the industry.&lt;/p&gt;&lt;p&gt;Since many of the projects I have worked on are still not released yet, I am not allowed to talk about them. But in my 3 years of working in the industry, I have worked for companies like Render Imagination, Volta, Share Creators, Mood Visuals, Porsche, and some smaller indie game productions. The projects were a mixture of AAA games, movies, and TV shows.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Production&lt;/h3&gt;&lt;p&gt;I started taking the Environment Design Mentorship with Sathish Kumar and therefore we all had to come up with a project that pushes our boundaries. I always struggled with the design aspects of my projects so I challenged myself by going for a topic where I have to come up with my own unique designs. &lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;The Story&lt;/h3&gt;&lt;p&gt;Greek mythology has always fascinated me. I loved Hercules when I was a kid (and still do) and the underworld of Hades was my favorite part of the story. When I was looking for a topic, my boyfriend brought the great idea to get inspired by the whole mythology. So I started listening to all the stories of the Greek gods and goddesses.&lt;/p&gt;&lt;p&gt;They were all great, but Psyche's story caught my attention the most. The fact that it was about this innocent girl, ready to go through the underworld and even risk her own death for this minimal chance to win back her beloved Eros, inspired me a lot.&lt;/p&gt;&lt;p&gt;I wanted to catch these emotions. The fear, the uncertainties but also her determination and courage to do this final task, given by Aphrodite. I started gathering all kinds of references. The greek architecture, the pillar types, clothing, statues, and also fine art paintings from the mythologies helped me a lot to get a feel for it. Also, I watched Disney's Hercules again and had a closer look at their design choices and what shots they took to emphasize the underworld.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Asset Creation&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;The Pillar&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Following all the mythologies, the underworld is full of dead souls who are doomed to a single task for eternity. That was the main inspiration for the pillars in my first shot. It's about the souls who are forced to carry the weight of the underworld on their shoulders. After I sketched out different shape designs, I went into 3D and started building them!&lt;/p&gt;&lt;p&gt;I started by creating a basic pillar shape. Then I went to Character Creator to get the pose for the statue. I also used Marvelous Designer to create some sort of chiton. Since you always try to be as fast and efficient as you can as a concept artist, I use kitbash whenever I can to speed up my workflow. &lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;In this case, I found an awesome horror pack by Dystopia Interactive. They have some cool sculpted body parts which you can use to get this horror look.&lt;/p&gt;&lt;p&gt;So I bashed it onto my existing model and sculpted it on top of it to make it unique. After that, I added elements that visualize the story of those pillars like damaged, cracked stone, shackles, and some greek ornaments which I made with IMM and Curve Brushes.&lt;/p&gt;&lt;p&gt;The details of the dead bodies on the bottom are not sculpted directly onto the pillar. I found a very cool base of this on the internet, changed it, and placed it where I wanted it to be.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;strong&gt;The Gate&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The Gate was done the same way as the Pillars. I just sculpted the base in ZBrush and added some details to make it look old and more realistic. I was creating alpha in Photoshop to get those Details on the door. For this, I just screengrabbed my model from the Viewport, put it in Photoshop, and drew the pattern (with mirroring on) until I got the results I wanted. Then I saved it as a black and white image and imported it as an alpha in ZBrush, and done!&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;For the ornaments, I used IMM Brushes again. I found the statues on Sketchfab and decided to add them since they fit perfectly into my scene.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;strong&gt;The Dead Bodies&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;They were super easy to do – I just took a skeleton base mesh and posed it in a few different poses. To make each one look unique, I took the horror pack again and replaced some body parts, so they became a mixture between bones and flesh. Then I placed them around in my scene. I always made sure to have big, medium, and small shapes so I also created sets of smaller skulls and bones.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;The Scale of the Scene&lt;/h3&gt;&lt;p&gt;To make this whole scene look giant and intimidating I did not only use the scale of the pillars in comparison to the character as a way to show it but also camera settings and lighting.&lt;/p&gt;&lt;p&gt;To get a sense of scale in a movie shot, they often use long lenses to have the real-world scale in the frame. It makes the whole thing kind of flat but that's what I love about it, to be honest. It gives the image a graphical look. In the following example, I'll show you how this would look with a wider lens (35mm). You get more depth but lose the real scale because the wider you go, the more it distorts the objects.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;The light also matters. If you have an object in the scene that is supposed to look giant, you can simply highlight a few parts that you want to emphasize and then let the rest fade to a darker value. Maybe add some volumetrics as well to make it disappear in the distance. Volumetrics also help to give it this atmospheric desaturated look which is great for indicating scale and distance!&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Character Pipeline&lt;/h3&gt;&lt;p&gt;For my characters, I usually have the same pipeline in every project. I use Character Creator a lot to get my Base done. The SkinGen plugin made a big difference for me because it gave them a super realistic look compared to Daz 3D and other providers.&lt;/p&gt;&lt;p&gt;I normally spend some time inside Character Creator to bring my character as close as I can and then move on to Marvelous Designer. This program also became my absolute favorite when it comes to clothing and fabric simulation. Because I was learning how to sew and draw my own cutting pattern, I was excited to be able to do it digitally.&lt;/p&gt;&lt;p&gt;I don't do it all from scratch every time. Marvelous Designer makes it possible to save your patterns and clothes into your personal library so I have collected a lot of self-made and purchased clothes over time.&lt;/p&gt;&lt;p&gt;For Psyche, I was jumping into ZBrush after finishing simulating her dress because I had something very specific in mind and also needed to create her jewelry.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;After I was done with her look, I moved on to the textures. In terms of skin, I had a good base because of Character Creator which I didn't really have to tweak. Only the golden makeup on her face was added on top.&lt;/p&gt;&lt;p&gt;For the rest, I usually use Substance 3D Painter or I'm setting up the shaders directly in Blender.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;I really have to mention this great plugin that fastened my Blender/Substance 3D Painter workflow a lot. It is called &lt;a href="https://xolotlstudio.gumroad.com/l/fTRFN" target="_blank"&gt;Substance Painter Live Link&lt;/a&gt; by Xolotl Studio. With this live link I can have my model in Blender and send it with one click to SP, then edit my textures and simply update the model in Blender to see how it looks in my final render.&lt;/p&gt;&lt;p&gt;This way you don't have these steps in between where you have to save your textures and assign them in Blender. A huge time saver! With this tool, I was jumping back and forth texturing her skin details like the golden stripe and the little dots on her face.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;Also, the blood on her cape which you can see in the shot below was simply a paint layer in Substance 3D Painter. I painted blood on the parts that would've gotten wet when she got out of the boat.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;For her eyes, I have built a shader in Blender.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Rendering and Lighting&lt;/h3&gt;&lt;p&gt;For the whole setup, I tried to stay as close as possible to my initial sketch. This is something that I feel is super important. To match the sketch, I created a reference image plane in Blender and placed it right in front of the camera. This way I can easily match my initial composition and lighting.&lt;/p&gt;&lt;p&gt;For the mood and the lighting in general I started off with volumetrics. This creates a very moody base for the scene. I usually light my scenes with area and spotlights because I feel like they give me a lot of control but in addition to that, I started using a technique that really helped me get these results.&lt;/p&gt;&lt;p&gt;For this technique, I have used a spotlight to light a part of the pillar. In the shader editor, I added a texture image with a reference that has the colors and light/dark values I want. In this case, I have used a shot from King Arthur as it fits exactly the mood I wanted to create.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;It is important to have a mapping note attached and play around with the location values first in order to see results. Adding a value node helps to increase or decrease the scale of the value variations.&lt;/p&gt;&lt;p&gt;For the dark and bright areas, I mainly focused on readability. Shapes are very important when it comes to this so I made sure to either have them dark with a bright background or the other way around. On the parts where I don't want much attention, I let the shapes fade into the background value. This is a good way to lead the eye, by creating contrast where we want the viewer to look.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;This project took me about 4 weeks to complete. In the mentorship, we had the first 5 weeks focusing on our 2D skills. Then we had to come up with some ideas and pitch them to Sathish, our mentor. After that, he gave us a deadline and we started creating our project with updates and meet-ups every weekend.&lt;/p&gt;&lt;p&gt;The main challenge for me was to stay free in my mind. I didn't want to produce what's already existing on ArtStation. It was important for me to get as much inspiration from the real world instead of other artworks and I think this was the key to designing a world that you want to explore more and dive deeper into. Something you don't see every day.&lt;/p&gt;&lt;p&gt;At this point, I want to thank Sathish Kumar again for being a great mentor and friend during that time. Without him, I would have never stepped out of my comfort zone like I did, not only with my project but also as an artist. I learned a lot during those weeks about how to grow as an artist and also as a person. I am truly thankful for this experience. I can absolutely recommend this course to every single artist out there.&lt;/p&gt;&lt;p&gt;If you enjoy my art, feel free to follow me for updates on &lt;a href="https://www.artstation.com/niinalina" target="_blank"&gt;ArtStation&lt;/a&gt; or &lt;a href="https://www.instagram.com/nina_leinwatter_art/" target="_blank"&gt;Instagram&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content><textplain>Nina Leinwatter has shared her experience and thoughts on the Environment Design Mentorship course, talked about the story behind the Gates of the Underworld project, and gave some useful tips on asset creation, rendering, and lighting in Blender.IntroductionMy Name is Nina Leinwatter, I am from Vienna, Austria. After I got my high school diploma in graphic design and fashion, I was studying Game Art and 3D Animation at the SAE Institute in Vienna. Then I started to focus on my portfolio and figured out what field of digital art I want to work in later on.I was taking courses such as Keyframe Illustration for Production by Ricardo Lima, Concept Art for Film by Jama Jurabaev, and Environment Design Mentorship by Sathish Kumar. They all helped me a lot to choose my path and connect with people from the industry.Since many of the projects I have worked on are still not released yet, I am not allowed to talk about them. But in my 3 years of working in the industry, I have worked for companies like Render Imagination, Volta, Share Creators, Mood Visuals, Porsche, and some smaller indie game productions. The projects were a mixture of AAA games, movies, and TV shows.ProductionI started taking the Environment Design Mentorship with Sathish Kumar and therefore we all had to come up with a project that pushes our boundaries. I always struggled with the design aspects of my projects so I challenged myself by going for a topic where I have to come up with my own unique designs. The StoryGreek mythology has always fascinated me. I loved Hercules when I was a kid (and still do) and the underworld of Hades was my favorite part of the story. When I was looking for a topic, my boyfriend brought the great idea to get inspired by the whole mythology. So I started listening to all the stories of the Greek gods and goddesses.They were all great, but Psyche's story caught my attention the most. The fact that it was about this innocent girl, ready to go through the underworld and even risk her own death for this minimal chance to win back her beloved Eros, inspired me a lot.I wanted to catch these emotions. The fear, the uncertainties but also her determination and courage to do this final task, given by Aphrodite. I started gathering all kinds of references. The greek architecture, the pillar types, clothing, statues, and also fine art paintings from the mythologies helped me a lot to get a feel for it. Also, I watched Disney's Hercules again and had a closer look at their design choices and what shots they took to emphasize the underworld.Asset CreationThe PillarFollowing all the mythologies, the underworld is full of dead souls who are doomed to a single task for eternity. That was the main inspiration for the pillars in my first shot. It's about the souls who are forced to carry the weight of the underworld on their shoulders. After I sketched out different shape designs, I went into 3D and started building them!I started by creating a basic pillar shape. Then I went to Character Creator to get the pose for the statue. I also used Marvelous Designer to create some sort of chiton. Since you always try to be as fast and efficient as you can as a concept artist, I use kitbash whenever I can to speed up my workflow. In this case, I found an awesome horror pack by Dystopia Interactive. They have some cool sculpted body parts which you can use to get this horror look.So I bashed it onto my existing model and sculpted it on top of it to make it unique. After that, I added elements that visualize the story of those pillars like damaged, cracked stone, shackles, and some greek ornaments which I made with IMM and Curve Brushes.The details of the dead bodies on the bottom are not sculpted directly onto the pillar. I found a very cool base of this on the internet, changed it, and placed it where I wanted it to be.The GateThe Gate was done the same way as the Pillars. I just sculpted the base in ZBrush and added some details to make it look old and more realistic. I was creating alpha in Photoshop to get those Details on the door. For this, I just screengrabbed my model from the Viewport, put it in Photoshop, and drew the pattern (with mirroring on) until I got the results I wanted. Then I saved it as a black and white image and imported it as an alpha in ZBrush, and done!For the ornaments, I used IMM Brushes again. I found the statues on Sketchfab and decided to add them since they fit perfectly into my scene.The Dead BodiesThey were super easy to do – I just took a skeleton base mesh and posed it in a few different poses. To make each one look unique, I took the horror pack again and replaced some body parts, so they became a mixture between bones and flesh. Then I placed them around in my scene. I always made sure to have big, medium, and small shapes so I also created sets of smaller skulls and bones.The Scale of the SceneTo make this whole scene look giant and intimidating I did not only use the scale of the pillars in comparison to the character as a way to show it but also camera settings and lighting.To get a sense of scale in a movie shot, they often use long lenses to have the real-world scale in the frame. It makes the whole thing kind of flat but that's what I love about it, to be honest. It gives the image a graphical look. In the following example, I'll show you how this would look with a wider lens (35mm). You get more depth but lose the real scale because the wider you go, the more it distorts the objects.The light also matters. If you have an object in the scene that is supposed to look giant, you can simply highlight a few parts that you want to emphasize and then let the rest fade to a darker value. Maybe add some volumetrics as well to make it disappear in the distance. Volumetrics also help to give it this atmospheric desaturated look which is great for indicating scale and distance!Character PipelineFor my characters, I usually have the same pipeline in every project. I use Character Creator a lot to get my Base done. The SkinGen plugin made a big difference for me because it gave them a super realistic look compared to Daz 3D and other providers.I normally spend some time inside Character Creator to bring my character as close as I can and then move on to Marvelous Designer. This program also became my absolute favorite when it comes to clothing and fabric simulation. Because I was learning how to sew and draw my own cutting pattern, I was excited to be able to do it digitally.I don't do it all from scratch every time. Marvelous Designer makes it possible to save your patterns and clothes into your personal library so I have collected a lot of self-made and purchased clothes over time.For Psyche, I was jumping into ZBrush after finishing simulating her dress because I had something very specific in mind and also needed to create her jewelry.After I was done with her look, I moved on to the textures. In terms of skin, I had a good base because of Character Creator which I didn't really have to tweak. Only the golden makeup on her face was added on top.For the rest, I usually use Substance 3D Painter or I'm setting up the shaders directly in Blender.I really have to mention this great plugin that fastened my Blender/Substance 3D Painter workflow a lot. It is called Substance Painter Live Link by Xolotl Studio. With this live link I can have my model in Blender and send it with one click to SP, then edit my textures and simply update the model in Blender to see how it looks in my final render.This way you don't have these steps in between where you have to save your textures and assign them in Blender. A huge time saver! With this tool, I was jumping back and forth texturing her skin details like the golden stripe and the little dots on her face.Also, the blood on her cape which you can see in the shot below was simply a paint layer in Substance 3D Painter. I painted blood on the parts that would've gotten wet when she got out of the boat.For her eyes, I have built a shader in Blender.Rendering and LightingFor the whole setup, I tried to stay as close as possible to my initial sketch. This is something that I feel is super important. To match the sketch, I created a reference image plane in Blender and placed it right in front of the camera. This way I can easily match my initial composition and lighting.For the mood and the lighting in general I started off with volumetrics. This creates a very moody base for the scene. I usually light my scenes with area and spotlights because I feel like they give me a lot of control but in addition to that, I started using a technique that really helped me get these results.For this technique, I have used a spotlight to light a part of the pillar. In the shader editor, I added a texture image with a reference that has the colors and light/dark values I want. In this case, I have used a shot from King Arthur as it fits exactly the mood I wanted to create.It is important to have a mapping note attached and play around with the location values first in order to see results. Adding a value node helps to increase or decrease the scale of the value variations.For the dark and bright areas, I mainly focused on readability. Shapes are very important when it comes to this so I made sure to either have them dark with a bright background or the other way around. On the parts where I don't want much attention, I let the shapes fade into the background value. This is a good way to lead the eye, by creating contrast where we want the viewer to look.ConclusionThis project took me about 4 weeks to complete. In the mentorship, we had the first 5 weeks focusing on our 2D skills. Then we had to come up with some ideas and pitch them to Sathish, our mentor. After that, he gave us a deadline and we started creating our project with updates and meet-ups every weekend.The main challenge for me was to stay free in my mind. I didn't want to produce what's already existing on ArtStation. It was important for me to get as much inspiration from the real world instead of other artworks and I think this was the key to designing a world that you want to explore more and dive deeper into. Something you don't see every day.At this point, I want to thank Sathish Kumar again for being a great mentor and friend during that time. Without him, I would have never stepped out of my comfort zone like I did, not only with my project but also as an artist. I learned a lot during those weeks about how to grow as an artist and also as a person. I am truly thankful for this experience. I can absolutely recommend this course to every single artist out there.If you enjoy my art, feel free to follow me for updates on ArtStation or Instagram.</textplain><image>https://cdn.80.lv/api/upload/meta/20131/images/62829748c8ab2/contain_440x220.jpg</image></item><item><title>Google is Adding AI-Powered Studio Lighting to Video Calls</title><link>https://80.lv/articles/google-is-adding-ai-powered-studio-lighting-to-video-calls</link><created_at>2022-11-03T07:03:34+00:00</created_at><description>Meeting over video calls has been a new norm for quite some time now. How often do your eyes stray to your own image in the calls? Well, Google is here to make looking at yourself easier with help of machine...</description><content>&lt;div&gt;&lt;div&gt;&lt;p&gt;&lt;iframe src="https://www.youtube.com/embed/CWTm0ccfZe4" frameborder="0" allow="encrypted-media; gyroscope; picture-in-picture"&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;Meeting over video calls has been a new norm for quite some time now. How often do your eyes stray to your own image in the calls? Well, Google is here to make looking at yourself easier with help of machine learning.&lt;/p&gt;&lt;p&gt;At Google I/O, the company revealed several handy features that make your video quality better. First of all, Portrait restore improves the quality by addressing issues caused by low light, low-quality webcams, or poor network connectivity.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;Another important part of any image is lighting. Google is working on Portrait light, which simulates studio-quality lighting, and you can adjust the light position and brightness.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;Seems that the new features are still in development and will be coming to Google Meet later. Check out what the company has in store &lt;a href="https://blog.google/products/workspace/hybrid-work-ai-tools-io-22/" target="_blank"&gt;here&lt;/a&gt; and don't forget to join &lt;a href="https://www.reddit.com/r/80lv/" target="_blank"&gt;our new Reddit page&lt;/a&gt;, &lt;a href="https://t.me/LevelEightyNews" target="_blank"&gt;our new Telegram channel&lt;/a&gt;, follow us on &lt;a href="https://www.instagram.com/eighty_level/" target="_blank"&gt;Instagram&lt;/a&gt; and &lt;a href="https://twitter.com/80Level" target="_blank"&gt;Twitter&lt;/a&gt;, where we are sharing breakdowns, the latest news, awesome artworks, and more.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content><textplain>Meeting over video calls has been a new norm for quite some time now. How often do your eyes stray to your own image in the calls? Well, Google is here to make looking at yourself easier with help of machine learning.At Google I/O, the company revealed several handy features that make your video quality better. First of all, Portrait restore improves the quality by addressing issues caused by low light, low-quality webcams, or poor network connectivity.Another important part of any image is lighting. Google is working on Portrait light, which simulates studio-quality lighting, and you can adjust the light position and brightness.Seems that the new features are still in development and will be coming to Google Meet later. Check out what the company has in store here and don't forget to join our new Reddit page, our new Telegram channel, follow us on Instagram and Twitter, where we are sharing breakdowns, the latest news, awesome artworks, and more.</textplain><image>https://cdn.80.lv/api/upload/meta/20123/images/628233c181c17/contain_440x220.jpg</image></item><item><title>This Neural Network Creates 3D Scenes From 2D Images </title><link>https://80.lv/articles/this-neural-network-creates-3d-scenes-from-2d-images</link><created_at>2022-11-03T08:39:43+00:00</created_at><description>Check out a powerful neural network that renders 3D scenes from sets of 2D images, letting you explore spaces as 3D environments. The process is straightforward: you just prepare a set of images and then...</description><content>&lt;div&gt;&lt;p&gt;Check out a powerful neural network that renders 3D scenes from sets of 2D images, letting you explore spaces as 3D environments. The process is straightforward: you just prepare a set of images and then upload it for the model to study. Then, using a set of points, it chooses the right angle, adds depth, and deals with missing details without producing too many artifacts.&lt;/p&gt;&lt;p&gt;The system works with complex objects and can even recreate vegetation or intricate objects like railings on a staircase, or ornaments.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;What is more, the model supports style transfer so you can recreate paintings as 3D scenes or apply a given style to a 3D environment. Where would you use this model? How can the neural network be used in games? Personally, I would love to recreate a couple of scenes from my favorite TV shows and then assemble them as a VR app.&lt;/p&gt;&lt;p&gt;You can find the paper and the files &lt;a href="https://repo-sam.inria.fr/fungraph/differentiable-multi-view/" target="_blank"&gt;here&lt;/a&gt;. Don't forget to join &lt;a href="https://www.reddit.com/r/80lv/" target="_blank"&gt;&lt;span&gt;our new Reddit page&lt;/span&gt;&lt;/a&gt;, &lt;a href="https://t.me/LevelEightyNews" target="_blank"&gt;&lt;span&gt;our new Telegram channel&lt;/span&gt;&lt;/a&gt;, follow us on &lt;a href="https://www.instagram.com/80.lv/" target="_blank"&gt;&lt;span&gt;Instagram&lt;/span&gt;&lt;/a&gt; and &lt;a href="https://twitter.com/80Level" target="_blank"&gt;&lt;span&gt;Twitter&lt;/span&gt;&lt;/a&gt;, where we are sharing breakdowns, the latest news, awesome artworks, and more. &lt;/p&gt;&lt;/div&gt;</content><textplain>Check out a powerful neural network that renders 3D scenes from sets of 2D images, letting you explore spaces as 3D environments. The process is straightforward: you just prepare a set of images and then upload it for the model to study. Then, using a set of points, it chooses the right angle, adds depth, and deals with missing details without producing too many artifacts.The system works with complex objects and can even recreate vegetation or intricate objects like railings on a staircase, or ornaments.What is more, the model supports style transfer so you can recreate paintings as 3D scenes or apply a given style to a 3D environment. Where would you use this model? How can the neural network be used in games? Personally, I would love to recreate a couple of scenes from my favorite TV shows and then assemble them as a VR app.You can find the paper and the files here. Don't forget to join our new Reddit page, our new Telegram channel, follow us on Instagram and Twitter, where we are sharing breakdowns, the latest news, awesome artworks, and more. </textplain><image>https://cdn.80.lv/api/upload/meta/17183/images/613e3f44014c6/contain_440x220.jpg</image></item><item><title>Xbox Boss Responded to the Starfield and Redfall Delays</title><link>https://80.lv/articles/xbox-boss-responded-to-the-starfield-and-redfall-delays</link><created_at>2022-11-03T07:04:32+00:00</created_at><description>Phil Spencer commented on the news of Bethesda's decision to push the launches of Starfield and Redfall to the first half of 2023 saying that Xbox "will continue to work to better meet expectations". He shared that he fully supports the delay of both titles, but promised to listen to players' feedback.</description><content>&lt;div&gt;&lt;div&gt;&lt;p&gt;Phil Spencer commented on the news of Bethesda's decision to push the launches of Starfield and Redfall to the first half of 2023 saying that Xbox "will continue to work to better meet expectations". He shared that he fully supports the delay of both titles, but promised to listen to players' feedback.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;p&gt;Xbox head Phil Spencer recently took to Twitter to comment on the news of Bethesda's decision to delay the launches of the highly anticipated sci-fi RPG Starfield and Arkane Studio's vampire shooter Redfall saying that Xbox "will continue to work to better meet [fans'] expectations".&lt;/p&gt;&lt;p&gt;On May 12, Bethesda &lt;a href="https://twitter.com/bethesda/status/1524721132720566272" target="_blank"&gt;announced&lt;/a&gt; that Redfall and Starfield had been pushed to the first half of 2023. The publisher noted that both development teams – Arkane Austin and Bethesda Game Studios – want to deliver the "most polished versions" of the games so as not to let the fans down.&lt;/p&gt;&lt;p&gt;Redfall was originally scheduled to release in the summer of 2022, while Starfield was scheduled to launch on November 11, 2022. Previously, the developers &lt;a href="https://www.washingtonpost.com/video-games/2021/06/13/starfield-bethesda/" target="_blank"&gt;claimed&lt;/a&gt; that they would not have to postpone the release of the space RPG, and Microsoft promised it would deliver a first-party game to Game Pass "at least" once a quarter. So, the fans are, obviously, not happy with the decision.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;&lt;img src /&gt;&lt;/p&gt;&lt;h3&gt;Starfield&lt;/h3&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;Phil Spencer addressed the news on his Twitter account noting that such decisions are always difficult for teams, but personally he fully supports the delay of both titles in order to give the studios the necessary time. "While I fully support giving teams time to release these great games when they are ready, we hear the feedback," Spencer wrote. "Delivering quality &amp;amp; consistency is expected, we will continue to work to better meet those expectations."&lt;/p&gt;&lt;p&gt;Both games are now due out in the first half of 2023 on PC and Xbox. The two titles will be the first Bethesda games since Microsoft acquired Bethesda for $7.5 billion that will not be released on PlayStation, but to be added on day one to the Xbox Game Pass library.&lt;/p&gt;&lt;p&gt;What do you think of the delay? Is it a good idea to postpone launches in order to deliver a higher quality game? Share your thoughts in the comments below and don't forget to join &lt;a href="https://www.reddit.com/r/80lv/" target="_blank"&gt;our new Reddit page&lt;/a&gt;, &lt;a href="https://t.me/LevelEightyNews" target="_blank"&gt;our new Telegram channel&lt;/a&gt;, follow us on &lt;a href="https://www.instagram.com/eighty_level/" target="_blank"&gt;Instagram&lt;/a&gt; and &lt;a href="https://twitter.com/80Level" target="_blank"&gt;Twitter&lt;/a&gt;, where we are sharing breakdowns, the latest news, awesome artworks, and more.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content><textplain>Phil Spencer commented on the news of Bethesda's decision to push the launches of Starfield and Redfall to the first half of 2023 saying that Xbox "will continue to work to better meet expectations". He shared that he fully supports the delay of both titles, but promised to listen to players' feedback.Xbox head Phil Spencer recently took to Twitter to comment on the news of Bethesda's decision to delay the launches of the highly anticipated sci-fi RPG Starfield and Arkane Studio's vampire shooter Redfall saying that Xbox "will continue to work to better meet [fans'] expectations".On May 12, Bethesda announced that Redfall and Starfield had been pushed to the first half of 2023. The publisher noted that both development teams – Arkane Austin and Bethesda Game Studios – want to deliver the "most polished versions" of the games so as not to let the fans down.Redfall was originally scheduled to release in the summer of 2022, while Starfield was scheduled to launch on November 11, 2022. Previously, the developers claimed that they would not have to postpone the release of the space RPG, and Microsoft promised it would deliver a first-party game to Game Pass "at least" once a quarter. So, the fans are, obviously, not happy with the decision.StarfieldPhil Spencer addressed the news on his Twitter account noting that such decisions are always difficult for teams, but personally he fully supports the delay of both titles in order to give the studios the necessary time. "While I fully support giving teams time to release these great games when they are ready, we hear the feedback," Spencer wrote. "Delivering quality &amp;amp; consistency is expected, we will continue to work to better meet those expectations."Both games are now due out in the first half of 2023 on PC and Xbox. The two titles will be the first Bethesda games since Microsoft acquired Bethesda for $7.5 billion that will not be released on PlayStation, but to be added on day one to the Xbox Game Pass library.What do you think of the delay? Is it a good idea to postpone launches in order to deliver a higher quality game? Share your thoughts in the comments below and don't forget to join our new Reddit page, our new Telegram channel, follow us on Instagram and Twitter, where we are sharing breakdowns, the latest news, awesome artworks, and more.</textplain><image>https://cdn.80.lv/api/upload/meta/20117/images/628205e8e327b/contain_440x220.jpg</image></item><item><title>Cool Cyberpunk Animation Made in Krita &amp; Blender</title><link>https://80.lv/articles/cool-cyberpunk-animation-made-in-krita-blender</link><created_at>2022-11-03T08:38:55+00:00</created_at><description>Concept Artist and Animator known as VictoryLuode has revealed a great new animation titled Chiba Castle. The animation depicts a stunning cyberpunk environment, filled to the brim with eye-catching details...</description><content>&lt;div&gt;&lt;p&gt;Concept Artist and Animator known as VictoryLuode has revealed a great new animation titled Chiba Castle. The animation depicts a stunning cyberpunk environment, filled to the brim with eye-catching details and small references scattered all over the scene. According to the artist, this entire project was made using Blender and Krita. What's more, the artist has also released a more-than-an-hour long timelapse process video that shows all the stages of the working process behind the environment. You can check out the animation and the timelapse down below.&lt;/p&gt;&lt;/div&gt;</content><textplain>Concept Artist and Animator known as VictoryLuode has revealed a great new animation titled Chiba Castle. The animation depicts a stunning cyberpunk environment, filled to the brim with eye-catching details and small references scattered all over the scene. According to the artist, this entire project was made using Blender and Krita. What's more, the artist has also released a more-than-an-hour long timelapse process video that shows all the stages of the working process behind the environment. You can check out the animation and the timelapse down below.</textplain><image>https://cdn.80.lv/api/upload/meta/20111/images/6281d7c92bc6b/contain_440x220.jpg</image></item><item><title>Photoshop to Support Substance Files</title><link>https://80.lv/articles/photoshop-to-support-substance-files</link><created_at>2022-11-03T07:03:12+00:00</created_at><description>According to Adobe, the software's beta build already has the new feature.Adobe announced that the upcoming version of Photoshop will come with native support for Substance 3D files, meaning Substance users...</description><content>&lt;div&gt;&lt;div&gt;&lt;p&gt;According to Adobe, the software's beta build already has the new feature.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;Adobe announced that the upcoming version of &lt;a href="https://adobe.prf.hn/click/camref:1100lriTb/destination:https%3A%2F%2Fwww.adobe.com%2Fproducts%2Fphotoshop.html" target="_blank"&gt;Photoshop&lt;/a&gt; will come with native support for Substance 3D files, meaning Substance users will soon be able to import procedural materials in .sbsar format into the software. According to the announcement, the new feature has already been added to Photoshop's beta build and can be tried and tested by Creative Cloud subscribers.&lt;/p&gt;&lt;p&gt;Unfortunately, not much has been revealed about the new feature's functionality. The team has only commented that the upcoming update will come with "parameters and lighting configuration". Adobe's Product Manager Jeanette Matthews, however, also added that it will soon be possible to "use the Substance files on any 2D layer as a 'fill'".&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content><textplain>According to Adobe, the software's beta build already has the new feature.Adobe announced that the upcoming version of Photoshop will come with native support for Substance 3D files, meaning Substance users will soon be able to import procedural materials in .sbsar format into the software. According to the announcement, the new feature has already been added to Photoshop's beta build and can be tried and tested by Creative Cloud subscribers.Unfortunately, not much has been revealed about the new feature's functionality. The team has only commented that the upcoming update will come with "parameters and lighting configuration". Adobe's Product Manager Jeanette Matthews, however, also added that it will soon be possible to "use the Substance files on any 2D layer as a 'fill'".</textplain><image>https://cdn.80.lv/api/upload/meta/20132/images/62830c088254e/contain_440x220.jpg</image></item><item><title>GANcraft: Creating Photorealistic Images of 3D Block Worlds</title><link>https://80.lv/articles/gancraft-creating-photorealistic-images-of-3d-block-worlds</link><created_at>2022-11-03T08:39:56+00:00</created_at><description>Zekun Hao, Arun Mallya, Serge Belongie, and Ming-Yu Liu have presented NVIDIA GANcraft, an unsupervised neural rendering framework for generating photorealistic images of large 3D block worlds such as those...</description><content>&lt;div&gt;&lt;p&gt;Zekun Hao, Arun Mallya, Serge Belongie, and Ming-Yu Liu have presented NVIDIA GANcraft, an unsupervised neural rendering framework for generating photorealistic images of large 3D block worlds such as those created in Minecraft.&lt;/p&gt;&lt;p&gt;The method takes a semantic block world as input, where each block is assigned a label such as dirt, grass, tree, sand, or water. The world is represented as a continuous volumetric function and the model is trained to render view-consistent photorealistic images from arbitrary viewpoints.&lt;/p&gt;&lt;/div&gt;</content><textplain>Zekun Hao, Arun Mallya, Serge Belongie, and Ming-Yu Liu have presented NVIDIA GANcraft, an unsupervised neural rendering framework for generating photorealistic images of large 3D block worlds such as those created in Minecraft.The method takes a semantic block world as input, where each block is assigned a label such as dirt, grass, tree, sand, or water. The world is represented as a continuous volumetric function and the model is trained to render view-consistent photorealistic images from arbitrary viewpoints.</textplain><image>https://cdn.80.lv/api/upload/meta/17471/images/6168fdb7ee12b/contain_440x220.jpg</image></item></items></response>
